{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f105708-f6f6-49c6-8fa6-5380a2b1d1b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+--------+----------+---------+----------+--------------+---------------+\n|production_id|   date_id| time_id|turbine_id|status_id|wind_speed|wind_direction|energy_produced|\n+-------------+----------+--------+----------+---------+----------+--------------+---------------+\n|            1|2024-06-01|00-00-00|         1|        4|  18.44936|            SE|     1786.91843|\n|            2|2024-06-01|00-00-00|         2|        4|  16.34197|             E|     1156.38082|\n|            3|2024-06-01|00-00-00|         3|        4|  24.16424|             E|     1216.49768|\n|            4|2024-06-01|00-10-00|         1|        4|  13.03947|            NE|     1888.39496|\n|            5|2024-06-01|00-10-00|         2|        4|   4.00397|            SE|     1052.36231|\n|            6|2024-06-01|00-10-00|         3|        6|   5.22807|             N|            0.0|\n|            7|2024-06-01|00-20-00|         1|        6|  20.38065|            SE|            0.0|\n|            8|2024-06-01|00-20-00|         2|        4|  17.61423|            NW|     1459.88153|\n|            9|2024-06-01|00-20-00|         3|        4|  23.72739|            NW|      658.86141|\n|           10|2024-06-01|00-30-00|         1|        4|   6.28564|             E|      1184.2255|\n|           11|2024-06-01|00-30-00|         2|        4|   2.43217|            SW|       724.5123|\n|           12|2024-06-01|00-30-00|         3|        4|  10.88925|             S|     1522.73045|\n|           13|2024-06-01|00-40-00|         1|        4|  12.05173|            NE|      648.92053|\n|           14|2024-06-01|00-40-00|         2|        6|  17.02222|            SE|            0.0|\n|           15|2024-06-01|00-40-00|         3|        4|    4.9653|             W|       1411.746|\n|           16|2024-06-01|00-50-00|         1|        4|   2.88378|            SE|     1982.56076|\n|           17|2024-06-01|00-50-00|         2|        4|   6.80417|            SE|     1992.94935|\n|           18|2024-06-01|00-50-00|         3|        4|  11.53048|             S|      866.63839|\n|           19|2024-06-01|01-00-00|         1|        4|   4.53863|            SW|      975.80261|\n|           20|2024-06-01|01-00-00|         2|        4|  23.84013|            NW|     1731.48984|\n+-------------+----------+--------+----------+---------+----------+--------------+---------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "#load data from fact table\n",
    "df = spark.table(\"goldwind.fact\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ec7a311-2003-4f9c-852c-c5b75f893752",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\n# 1. Chargement des donn√©es GOLD et dimensions\\nfact = spark.table(\"goldwind.fact\")\\ndim_turbine = spark.table(\"goldwind.dim_turbine\")\\ndim_date = spark.table(\"goldwind.dim_date\")\\ndim_time = spark.table(\"goldwind.dim_time\")\\ndim_status = spark.table(\"goldwind.dim_status\")\\n\\n# 2. Jointures pour enrichir les donn√©es\\nfact_joined = fact     .join(dim_turbine, \"turbine_id\", \"left\")     .join(dim_date, fact.date_id == dim_date.date_id, \"left\")     .join(dim_time, fact.time_id == dim_time.time_id, \"left\")     .join(dim_status, \"status_id\", \"left\")\\n\\n# 3. Ajout de la colonne saison (facultatif mais utile) je devais normalement le mettre dans mon fichier silver lol\\nfrom pyspark.sql.functions import when, col\\n\\nfact_joined = fact_joined.withColumn(\\n    \"season\",\\n    when(col(\"month\").isin(12, 1, 2), \"winter\")\\n    .when(col(\"month\").isin(3, 4, 5), \"spring\")\\n    .when(col(\"month\").isin(6, 7, 8), \"summer\")\\n    .when(col(\"month\").isin(9, 10, 11), \"autumn\")\\n)\\n\\n# 4. Nettoyage et filtrage des donn√©es\\nfact_filtered = fact_joined.filter(col(\"energy_produced\") > 0).dropna(subset=[\\n    \"wind_speed\", \"wind_direction\", \"turbine_id\", \"status_id\", \"energy_produced\",\\n    \"hour\", \"month\", \"capacity\", \"responsible_department\"\\n])\\ndisplay(fact_filtered)\\n'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# üìò Notebook ML - Pr√©diction de l'√©nergie produite par les turbines (version enrichie avec toutes les dimensions)\n",
    "\n",
    "# 1. Chargement des donn√©es GOLD et dimensions\n",
    "fact = spark.table(\"goldwind.fact\")\n",
    "dim_turbine = spark.table(\"goldwind.dim_turbine\")\n",
    "dim_date = spark.table(\"goldwind.dim_date\")\n",
    "dim_time = spark.table(\"goldwind.dim_time\")\n",
    "dim_status = spark.table(\"goldwind.dim_status\")\n",
    "\n",
    "# 2. Jointures pour enrichir les donn√©es\n",
    "fact_joined = fact \\\n",
    "    .join(dim_turbine, \"turbine_id\", \"left\") \\\n",
    "    .join(dim_date, fact.date_id == dim_date.date_id, \"left\") \\\n",
    "    .join(dim_time, fact.time_id == dim_time.time_id, \"left\") \\\n",
    "    .join(dim_status, \"status_id\", \"left\")\n",
    "\n",
    "# 3. Ajout de la colonne saison (facultatif mais utile) je devais normalement le mettre dans mon fichier silver lol\n",
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "fact_joined = fact_joined.withColumn(\n",
    "    \"season\",\n",
    "    when(col(\"month\").isin(12, 1, 2), \"winter\")\n",
    "    .when(col(\"month\").isin(3, 4, 5), \"spring\")\n",
    "    .when(col(\"month\").isin(6, 7, 8), \"summer\")\n",
    "    .when(col(\"month\").isin(9, 10, 11), \"autumn\")\n",
    ")\n",
    "\n",
    "# 4. Nettoyage et filtrage des donn√©es\n",
    "fact_filtered = fact_joined.filter(col(\"energy_produced\") > 0).dropna(subset=[\n",
    "    \"wind_speed\", \"wind_direction\", \"turbine_id\", \"status_id\", \"energy_produced\",\n",
    "    \"hour\", \"month\", \"capacity\", \"responsible_department\"\n",
    "])\n",
    "display(fact_filtered)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d8ecbdd3-8fbe-4dbc-a519-ec0c860bfb91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e5d2a9e-a12b-4e82-a36e-e848ce1e1908",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\n# 4. Nettoyage et filtrage des donn√©es\\nfact_filtered = fact_joined.filter(col(\"energy_produced\") > 0).dropna(subset=[\\n    \"wind_speed\", \"wind_direction\", \"turbine_id\", \"status_id\", \"energy_produced\",\\n    \"hour\", \"month\", \"capacity\", \"responsible_department\"\\n])\\n\\n# 5. Encodages n√©cessaires\\nfrom pyspark.ml.feature import StringIndexer, VectorAssembler\\n\\nindexer_dir = StringIndexer(inputCol=\"wind_direction\", outputCol=\"wind_dir_indexed\")\\nindexer_dept = StringIndexer(inputCol=\"responsible_department\", outputCol=\"responsible_dept_indexed\")\\nindexer_season = StringIndexer(inputCol=\"season\", outputCol=\"season_indexed\")\\n\\nindexed = indexer_dir.fit(fact_filtered).transform(fact_filtered)\\nindexed = indexer_dept.fit(indexed).transform(indexed)\\nindexed = indexer_season.fit(indexed).transform(indexed)\\n\\n# üîß Conversion de types pour VectorAssembler\\nindexed = indexed.withColumn(\"hour\", col(\"hour\").cast(\"int\"))\\nindexed = indexed.withColumn(\"month\", col(\"month\").cast(\"int\"))\\nindexed = indexed.withColumn(\"capacity\", col(\"capacity\").cast(\"int\"))\\n\\n# 6. Assemblage des features enrichies\\nassembler = VectorAssembler(\\n    inputCols=[\\n        \"wind_speed\",\\n        \"wind_dir_indexed\",\\n        \"turbine_id\",\\n        \"status_id\",\\n        \"hour\",\\n        \"month\",\\n        \"capacity\",\\n        \"responsible_dept_indexed\",\\n        \"season_indexed\"\\n    ],\\n    outputCol=\"features\"\\n)\\nfinal_data = assembler.transform(indexed).select(\"features\", \"energy_produced\")\\ndisplay(final_data)\\n'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# 4. Nettoyage et filtrage des donn√©es\n",
    "fact_filtered = fact_joined.filter(col(\"energy_produced\") > 0).dropna(subset=[\n",
    "    \"wind_speed\", \"wind_direction\", \"turbine_id\", \"status_id\", \"energy_produced\",\n",
    "    \"hour\", \"month\", \"capacity\", \"responsible_department\"\n",
    "])\n",
    "\n",
    "# 5. Encodages n√©cessaires\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "\n",
    "indexer_dir = StringIndexer(inputCol=\"wind_direction\", outputCol=\"wind_dir_indexed\")\n",
    "indexer_dept = StringIndexer(inputCol=\"responsible_department\", outputCol=\"responsible_dept_indexed\")\n",
    "indexer_season = StringIndexer(inputCol=\"season\", outputCol=\"season_indexed\")\n",
    "\n",
    "indexed = indexer_dir.fit(fact_filtered).transform(fact_filtered)\n",
    "indexed = indexer_dept.fit(indexed).transform(indexed)\n",
    "indexed = indexer_season.fit(indexed).transform(indexed)\n",
    "\n",
    "# üîß Conversion de types pour VectorAssembler\n",
    "indexed = indexed.withColumn(\"hour\", col(\"hour\").cast(\"int\"))\n",
    "indexed = indexed.withColumn(\"month\", col(\"month\").cast(\"int\"))\n",
    "indexed = indexed.withColumn(\"capacity\", col(\"capacity\").cast(\"int\"))\n",
    "\n",
    "# 6. Assemblage des features enrichies\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\n",
    "        \"wind_speed\",\n",
    "        \"wind_dir_indexed\",\n",
    "        \"turbine_id\",\n",
    "        \"status_id\",\n",
    "        \"hour\",\n",
    "        \"month\",\n",
    "        \"capacity\",\n",
    "        \"responsible_dept_indexed\",\n",
    "        \"season_indexed\"\n",
    "    ],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "final_data = assembler.transform(indexed).select(\"features\", \"energy_produced\")\n",
    "display(final_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e1558fd-57dc-49a1-ab1b-3fea2cb378ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 7. Split en train/test\n",
    "train_data, test_data = final_data.randomSplit([0.8, 0.2], seed=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ff830b7-662f-4b26-bb90-eba07c1959e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/04/22 12:26:53 INFO mlflow.spark: Inferring pip requirements by reloading the logged model from the databricks artifact repository, which can be time-consuming. To speed up, explicitly specify the conda_env or pip_requirements when calling log_model().\n2025/04/22 12:27:25 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: dbfs:/databricks/mlflow-tracking/7146a1d339d64ccfa347b14638dcb1e3/41f7bf8ff25643ad9cb6e78f3fb0ca1b/artifacts/model_LinearRegression/sparkml, flavor: spark). Fall back to return ['pyspark==3.5.0']. Set logging level to DEBUG to see the full traceback. \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä LinearRegression RMSE: 430.6956\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/04/22 12:27:36 INFO mlflow.spark: Inferring pip requirements by reloading the logged model from the databricks artifact repository, which can be time-consuming. To speed up, explicitly specify the conda_env or pip_requirements when calling log_model().\n2025/04/22 12:28:08 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: dbfs:/databricks/mlflow-tracking/7146a1d339d64ccfa347b14638dcb1e3/341bbbccd4e54259a7d63731ef9d72a4/artifacts/model_DecisionTree/sparkml, flavor: spark). Fall back to return ['pyspark==3.5.0']. Set logging level to DEBUG to see the full traceback. \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä DecisionTree RMSE: 437.4546\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/04/22 12:28:24 INFO mlflow.spark: Inferring pip requirements by reloading the logged model from the databricks artifact repository, which can be time-consuming. To speed up, explicitly specify the conda_env or pip_requirements when calling log_model().\n2025/04/22 12:28:56 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: dbfs:/databricks/mlflow-tracking/7146a1d339d64ccfa347b14638dcb1e3/f825afa308e74e97ac45cf50ad4f50c7/artifacts/model_RandomForest/sparkml, flavor: spark). Fall back to return ['pyspark==3.5.0']. Set logging level to DEBUG to see the full traceback. \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä RandomForest RMSE: 431.5084\n\n‚úÖ Meilleur mod√®le : LinearRegression avec RMSE = 430.6956\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 8. Entra√Ænement et comparaison de plusieurs mod√®les\n",
    "from pyspark.ml.regression import LinearRegression, DecisionTreeRegressor, RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "import mlflow\n",
    "import mlflow.spark\n",
    "\n",
    "models = {\n",
    "    \"LinearRegression\": LinearRegression(labelCol=\"energy_produced\", featuresCol=\"features\"),\n",
    "    \"DecisionTree\": DecisionTreeRegressor(labelCol=\"energy_produced\", featuresCol=\"features\"),\n",
    "    \"RandomForest\": RandomForestRegressor(labelCol=\"energy_produced\", featuresCol=\"features\")\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "for name, algo in models.items():\n",
    "    with mlflow.start_run(run_name=name):\n",
    "        pipeline = Pipeline(stages=[algo])\n",
    "        model = pipeline.fit(train_data)\n",
    "        predictions = model.transform(test_data)\n",
    "\n",
    "        evaluator = RegressionEvaluator(labelCol=\"energy_produced\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "        rmse = evaluator.evaluate(predictions)\n",
    "\n",
    "        mlflow.spark.log_model(model, f\"model_{name}\")\n",
    "        mlflow.log_metric(\"rmse\", rmse)\n",
    "\n",
    "        results.append((name, rmse))\n",
    "        print(f\"üìä {name} RMSE: {rmse:.4f}\")\n",
    "\n",
    "# 9. S√©lection du meilleur mod√®le\n",
    "best_model_name, best_rmse = sorted(results, key=lambda x: x[1])[0]\n",
    "print(f\"\\n‚úÖ Meilleur mod√®le : {best_model_name} avec RMSE = {best_rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a3b1fe8-e76b-48d4-944d-5529ffe9cf1b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä LinearRegression RMSE: 430.8846\nüìä DecisionTree RMSE: 426.2068\nüìä RandomForest RMSE: 428.5398\n\n‚úÖ Meilleur mod√®le : DecisionTree avec RMSE = 426.2068\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression, DecisionTreeRegressor, RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "models = {\n",
    "    \"LinearRegression\": LinearRegression(labelCol=\"energy_produced\", featuresCol=\"features\"),\n",
    "    \"DecisionTree\": DecisionTreeRegressor(labelCol=\"energy_produced\", featuresCol=\"features\"),\n",
    "    \"RandomForest\": RandomForestRegressor(labelCol=\"energy_produced\", featuresCol=\"features\")\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "for name, algo in models.items():\n",
    "    pipeline = Pipeline(stages=[algo])\n",
    "    model = pipeline.fit(train_data)\n",
    "    predictions = model.transform(test_data)\n",
    "    \n",
    "    evaluator = RegressionEvaluator(labelCol=\"energy_produced\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "    rmse = evaluator.evaluate(predictions)\n",
    "    \n",
    "    results.append((name, rmse))\n",
    "    print(f\"üìä {name} RMSE: {rmse:.4f}\")\n",
    "best_model_name, best_rmse = sorted(results, key=lambda x: x[1])[0]\n",
    "print(f\"\\n‚úÖ Meilleur mod√®le : {best_model_name} avec RMSE = {best_rmse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3927f31d-b7ee-4371-b73c-09ac1a8231a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# 10. R√©entra√Ænement du meilleur mod√®le sur toutes les donn√©es\n",
    "best_algo = models[best_model_name]\n",
    "best_pipeline = Pipeline(stages=[best_algo])\n",
    "best_model = best_pipeline.fit(final_data)\n",
    "\n",
    "# 11. Pr√©dictions compl√®tes (batch)\n",
    "full_predictions = best_model.transform(final_data)\n",
    "\n",
    "# 12. Sauvegarde des pr√©dictions en Delta table\n",
    "full_predictions.select(\"features\", \"prediction\").write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"goldwind.predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ce83e36-e838-4c8c-a873-09b2037fe46b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------+----------+---------+----+-----+--------+------------------------+--------------+------------------+\n|wind_speed|wind_dir_indexed|turbine_id|status_id|hour|month|capacity|responsible_dept_indexed|season_indexed|        prediction|\n+----------+----------------+----------+---------+----+-----+--------+------------------------+--------------+------------------+\n|  18.44936|             3.0|       1.0|      4.0| 0.0|  6.0|  2200.0|                     0.0|           0.0|1277.2685464523315|\n|  13.03947|             1.0|       1.0|      4.0| 0.0|  6.0|  2200.0|                     0.0|           0.0|1287.5236234046488|\n|   6.28564|             6.0|       1.0|      4.0| 0.0|  6.0|  2200.0|                     0.0|           0.0|  1271.95273040984|\n|  12.05173|             1.0|       1.0|      4.0| 0.0|  6.0|  2200.0|                     0.0|           0.0|1288.0139630972706|\n|   2.88378|             3.0|       1.0|      4.0| 0.0|  6.0|  2200.0|                     0.0|           0.0|1284.9957031055785|\n|   4.53863|             0.0|       1.0|      4.0| 1.0|  6.0|  2200.0|                     0.0|           0.0| 1294.876657785981|\n|  13.72136|             4.0|       1.0|      4.0| 1.0|  6.0|  2200.0|                     0.0|           0.0| 1275.179177858108|\n|   3.47539|             6.0|       1.0|      4.0| 1.0|  6.0|  2200.0|                     0.0|           0.0|1272.6960748869276|\n|  16.88501|             7.0|       1.0|      4.0| 1.0|  6.0|  2200.0|                     0.0|           0.0|1262.2544587697753|\n|  24.13034|             4.0|       1.0|      4.0| 1.0|  6.0|  2200.0|                     0.0|           0.0| 1270.011890865878|\n|  23.98992|             1.0|       1.0|      4.0| 2.0|  6.0|  2200.0|                     0.0|           0.0|   1280.7840640248|\n|   4.36586|             7.0|       1.0|      4.0| 2.0|  6.0|  2200.0|                     0.0|           0.0|1267.8175524133387|\n|  22.51424|             0.0|       1.0|      4.0| 2.0|  6.0|  2200.0|                     0.0|           0.0|1285.3013635491589|\n|  13.69126|             1.0|       1.0|      4.0| 2.0|  6.0|  2200.0|                     0.0|           0.0|1285.8965853146285|\n|  21.92679|             5.0|       1.0|      4.0| 2.0|  6.0|  2200.0|                     0.0|           0.0|1266.6693199744518|\n|   5.93269|             6.0|       1.0|      4.0| 2.0|  6.0|  2200.0|                     0.0|           0.0|1270.8244712522962|\n|  23.48692|             5.0|       1.0|      4.0| 3.0|  6.0|  2200.0|                     0.0|           0.0| 1265.243094741049|\n|  12.39851|             5.0|       1.0|      4.0| 3.0|  6.0|  2200.0|                     0.0|           0.0|1270.7476683647387|\n|   3.54674|             1.0|       1.0|      4.0| 3.0|  6.0|  2200.0|                     0.0|           0.0|1290.2808511854073|\n|  22.27646|             3.0|       1.0|      4.0| 3.0|  6.0|  2200.0|                     0.0|           0.0|1273.4134659860079|\n+----------+----------------+----------+---------+----+-----+--------+------------------------+--------------+------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "#vue lisble \n",
    "# 13. Vue lisible des pr√©dictions\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, DoubleType\n",
    "\n",
    "def extract_features(vector):\n",
    "    return vector.toArray().tolist()\n",
    "\n",
    "extract_udf = udf(extract_features, ArrayType(DoubleType()))\n",
    "\n",
    "df = full_predictions.withColumn(\"features_array\", extract_udf(\"features\"))\n",
    "df_lisible = df.selectExpr(\n",
    "    \"features_array[0] as wind_speed\",\n",
    "    \"features_array[1] as wind_dir_indexed\",\n",
    "    \"features_array[2] as turbine_id\",\n",
    "    \"features_array[3] as status_id\",\n",
    "    \"features_array[4] as hour\",\n",
    "    \"features_array[5] as month\",\n",
    "    \"features_array[6] as capacity\",\n",
    "    \"features_array[7] as responsible_dept_indexed\",\n",
    "    \"features_array[8] as season_indexed\",\n",
    "    \"prediction\"\n",
    ")\n",
    "\n",
    "# Affichage des pr√©dictions lisibles\n",
    "df_lisible.show()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5630859432516836,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Predictions_modele",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}